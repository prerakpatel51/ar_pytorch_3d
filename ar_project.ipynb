{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================================\n",
        "# Cell: Setup & Dependencies - Install PyTorch3D\n",
        "# ============================================================================\n",
        "\n",
        "import sys\n",
        "import subprocess\n",
        "import os\n",
        "import shutil\n",
        "from typing import Tuple\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"INSTALLATION & ENVIRONMENT SETUP\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Install base dependencies\n",
        "subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\",\n",
        "                \"torch\", \"torchvision\", \"numpy\", \"opencv-python\",\n",
        "                \"matplotlib\", \"scipy\", \"Pillow\"], check=False)\n",
        "\n",
        "# ============================================================================\n",
        "# Platform-aware PyTorch3D Installation Helper (embedded sse_env.py)\n",
        "# ============================================================================\n",
        "\n",
        "def _run(cmd: list, check: bool = True) -> int:\n",
        "    \"\"\"Run a command (list form). Returns exit code.\"\"\"\n",
        "    print(\"$\", \" \".join(cmd))\n",
        "    try:\n",
        "        return subprocess.run(cmd, check=check).returncode\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"Command failed with code {e.returncode}: {' '.join(cmd)}\")\n",
        "        if check:\n",
        "            raise\n",
        "        return e.returncode\n",
        "\n",
        "def _pip_install(*pkgs: str, extra_args: list = None, check: bool = True) -> int:\n",
        "    args = [sys.executable, \"-m\", \"pip\", \"install\"]\n",
        "    if extra_args:\n",
        "        args += extra_args\n",
        "    args += list(pkgs)\n",
        "    return _run(args, check=check)\n",
        "\n",
        "def _apt_available() -> bool:\n",
        "    return shutil.which(\"apt-get\") is not None\n",
        "\n",
        "def _sudo_prefix() -> list:\n",
        "    return [\"sudo\"] if shutil.which(\"sudo\") else []\n",
        "\n",
        "def _is_colab() -> bool:\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except Exception:\n",
        "        return \"content\" in os.getcwd()\n",
        "\n",
        "class PlatformManager:\n",
        "    def __init__(self):\n",
        "        self.platform, self.local_path = self.detect_platform()\n",
        "\n",
        "    @staticmethod\n",
        "    def mount_gdrive():\n",
        "        \"\"\"Mount Google Drive in Colab (noop elsewhere).\"\"\"\n",
        "        if _is_colab():\n",
        "            try:\n",
        "                from google.colab import drive\n",
        "                drive.mount('/content/drive')\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to mount Google Drive: {e}\")\n",
        "        else:\n",
        "            print(\"Google Drive mount is only applicable in Colab.\")\n",
        "\n",
        "    @staticmethod\n",
        "    def detect_platform() -> Tuple[str, str]:\n",
        "        \"\"\"Detect platform and return its name and the local path\"\"\"\n",
        "        computing_platform = 'LocalPC'\n",
        "\n",
        "        if os.getenv('RUNPOD_POD_ID'):\n",
        "            computing_platform = \"RunPod\"\n",
        "            print(\"Running on RunPod.\")\n",
        "            local_path = \"/workspace/\"\n",
        "        elif 'content' in str(os.getcwd()):\n",
        "            computing_platform = \"Colab\"\n",
        "            print(\"Running on Colab.\")\n",
        "            local_path = \"/content/\"\n",
        "        elif os.getenv(\"LIGHTNING_ARTIFACTS_DIR\"):\n",
        "            computing_platform = \"LightningAI\"\n",
        "            print(\"Running on Lightning AI Studio\")\n",
        "            local_path = os.getenv(\"LIGHTNING_ARTIFACTS_DIR\") + '/'\n",
        "        else:\n",
        "            local_path = os.getcwd() + '/'\n",
        "\n",
        "        return computing_platform, local_path\n",
        "\n",
        "class DependencyInstaller:\n",
        "    @staticmethod\n",
        "    def install_glut(computing_platform: str):\n",
        "        _pip_install(\"--upgrade\", \"pip\", extra_args=[], check=False)\n",
        "\n",
        "        if not _apt_available():\n",
        "            print(\"apt-get not available on this system; skipping GL/GLUT dev packages.\")\n",
        "            return\n",
        "\n",
        "        apt = _sudo_prefix() + [\"apt-get\"]\n",
        "        if computing_platform in {\"LightningAI\", \"RunPod\", \"Colab\", \"LocalPC\"}:\n",
        "            _run(apt + [\"-qq\", \"update\"], check=False)\n",
        "            _run(apt + [\"install\", \"-y\",\n",
        "                        \"freeglut3-dev\", \"libglew-dev\", \"libsdl2-dev\"], check=False)\n",
        "\n",
        "    @staticmethod\n",
        "    def install_opengl():\n",
        "        try:\n",
        "            _pip_install(\"PyOpenGL\", \"PyOpenGL_accelerate\", extra_args=[], check=True)\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to install OpenGL: {e}\")\n",
        "\n",
        "    @staticmethod\n",
        "    def get_pytorch3d_version_string() -> str:\n",
        "        \"\"\"Compose the official PyTorch3D wheel tag.\"\"\"\n",
        "        import torch\n",
        "        pyt_version_str = torch.__version__.split(\"+\")[0].replace(\".\", \"\")\n",
        "        cuda_ver = getattr(torch.version, \"cuda\", None)\n",
        "        if not cuda_ver:\n",
        "            return None\n",
        "        version_str = \"\".join([\n",
        "            f\"py3{sys.version_info.minor}_cu\",\n",
        "            cuda_ver.replace(\".\", \"\"),\n",
        "            f\"_pyt{pyt_version_str}\"\n",
        "        ])\n",
        "        return version_str\n",
        "\n",
        "class PyTorch3DInstaller:\n",
        "    def __init__(self, computing_platform: str, local_path: str):\n",
        "        self.platform = computing_platform\n",
        "        self.local_path = local_path\n",
        "\n",
        "    def install(self):\n",
        "        need_pytorch3d = False\n",
        "\n",
        "        _pip_install(\"--upgrade\", \"pip\", extra_args=[], check=False)\n",
        "        DependencyInstaller.install_glut(self.platform)\n",
        "        DependencyInstaller.install_opengl()\n",
        "\n",
        "        version_str = DependencyInstaller.get_pytorch3d_version_string()\n",
        "        print(f\"\\nPyTorch3D target wheel tag: {version_str}\\n\")\n",
        "\n",
        "        _pip_install(\"iopath\", extra_args=[], check=False)\n",
        "\n",
        "        if sys.platform.startswith(\"linux\") and version_str:\n",
        "            print(f\"Trying to install PyTorch3D wheel on {self.platform} (Linux).\")\n",
        "            try:\n",
        "                if self.platform in {\"RunPod\", \"LightningAI\"}:\n",
        "                    index_url = f\"https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\"\n",
        "                    _pip_install(\"pytorch3d\",\n",
        "                                 extra_args=[\"--no-index\", \"--no-cache-dir\", \"-f\", index_url],\n",
        "                                 check=True)\n",
        "\n",
        "                elif self.platform == \"Colab\":\n",
        "                    wheel_url = (\"https://www.dropbox.com/scl/fi/fqvlnyponcbekjd01omhj/\"\n",
        "                                 \"pytorch3d-0.7.8-cp312-cp312-linux_x86_64.whl\"\n",
        "                                 \"?rlkey=563mfx35rog42z1c8y7qn31sk&dl=1\")\n",
        "                    _pip_install(wheel_url, extra_args=[], check=True)\n",
        "\n",
        "                else:\n",
        "                    index_url = f\"https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/{version_str}/download.html\"\n",
        "                    _pip_install(\"pytorch3d\",\n",
        "                                 extra_args=[\"--no-index\", \"--no-cache-dir\", \"-f\", index_url],\n",
        "                                 check=True)\n",
        "            except Exception as e:\n",
        "                print(f\"Wheel install failed: {e}\")\n",
        "                need_pytorch3d = True\n",
        "        else:\n",
        "            need_pytorch3d = True\n",
        "\n",
        "        try:\n",
        "            import pytorch3d\n",
        "            print(\"âœ… PyTorch3D successfully installed!\")\n",
        "            need_pytorch3d = False\n",
        "        except Exception:\n",
        "            need_pytorch3d = True\n",
        "\n",
        "        if need_pytorch3d:\n",
        "            print(\"Falling back to source install for PyTorch3D (this may take a while).\")\n",
        "            _pip_install(\"ninja\", extra_args=[\"--root-user-action\", \"ignore\"], check=False)\n",
        "            _pip_install(\"git+https://github.com/facebookresearch/pytorch3d.git@stable\",\n",
        "                         extra_args=[], check=True)\n",
        "            try:\n",
        "                import pytorch3d\n",
        "            except Exception:\n",
        "                print(\"âŒ PyTorch3D failed to install from source.\")\n",
        "            else:\n",
        "                print(\"âœ… PyTorch3D successfully installed from source.\")\n",
        "\n",
        "# ============================================================================\n",
        "# Run PyTorch3D Installation\n",
        "# ============================================================================\n",
        "\n",
        "pm = PlatformManager()\n",
        "print(f\"\\nDetected platform: {pm.platform}\")\n",
        "print(f\"Local path: {pm.local_path}\")\n",
        "\n",
        "installer = PyTorch3DInstaller(pm.platform, pm.local_path)\n",
        "installer.install()\n",
        "\n",
        "print(\"\\nâœ“ Done.\")"
      ],
      "metadata": {
        "id": "W0lHuogCcGK8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d568c42c-09f3-48a8-ef1e-5bf958ef4d55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "INSTALLATION & ENVIRONMENT SETUP\n",
            "============================================================\n",
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "Working directory: /content\n",
            "Running on Colab.\n",
            "\n",
            "Detected platform: Colab\n",
            "Local path: /content/\n",
            "$ /usr/bin/python3 -m pip install --upgrade pip\n",
            "$ /usr/bin/python3 -m pip install --upgrade pip\n",
            "$ sudo apt-get -qq update\n",
            "$ sudo apt-get install -y freeglut3-dev libglew-dev libsdl2-dev\n",
            "$ /usr/bin/python3 -m pip install PyOpenGL PyOpenGL_accelerate\n",
            "\n",
            "PyTorch3D target wheel tag: py312_cu126_pyt280\n",
            "\n",
            "$ /usr/bin/python3 -m pip install iopath\n",
            "Trying to install PyTorch3D wheel on Colab (Linux).\n",
            "$ /usr/bin/python3 -m pip install https://www.dropbox.com/scl/fi/fqvlnyponcbekjd01omhj/pytorch3d-0.7.8-cp312-cp312-linux_x86_64.whl?rlkey=563mfx35rog42z1c8y7qn31sk&dl=1\n",
            "âœ… PyTorch3D successfully installed!\n",
            "\n",
            "âœ“ Done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Installing required packages...\")\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q fvcore iopath\n",
        "!pip install -q --no-index --no-cache-dir pytorch3d -f https://dl.fbaipublicfiles.com/pytorch3d/packaging/wheels/py38_cu117_pyt1110/download.html\n",
        "!pip install -q gradio opencv-python matplotlib numpy scipy\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from scipy.spatial.transform import Rotation as R\n",
        "import gradio as gr\n",
        "import io\n",
        "import base64\n",
        "\n",
        "# PyTorch3D imports\n",
        "from pytorch3d.structures import Meshes\n",
        "from pytorch3d.renderer import (\n",
        "    FoVPerspectiveCameras,\n",
        "    PerspectiveCameras,\n",
        "    PointLights,\n",
        "    DirectionalLights,\n",
        "    Materials,\n",
        "    RasterizationSettings,\n",
        "    MeshRenderer,\n",
        "    MeshRasterizer,\n",
        "    SoftPhongShader,\n",
        "    TexturesVertex,\n",
        "    BlendParams,\n",
        ")\n",
        "from pytorch3d.utils import ico_sphere\n",
        "from pytorch3d.io import load_objs_as_meshes\n",
        "\n",
        "print(\"âœ“ All packages installed successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzAVS19sW_cm",
        "outputId": "b942669d-118a-420f-995d-f94a3c175cec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing required packages...\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33m  DEPRECATION: Building 'fvcore' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'fvcore'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "âœ“ All packages installed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CameraPoseEstimator:\n",
        "    \"\"\"\n",
        "    Estimates camera pose from a planar object using perspective transformation.\n",
        "    Based on Assignment 2 concepts - extended for AR applications.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_width, image_height, focal_length=None):\n",
        "        \"\"\"\n",
        "        Initialize pose estimator\n",
        "\n",
        "        Args:\n",
        "            image_width: Width of the image in pixels\n",
        "            image_height: Height of the image in pixels\n",
        "            focal_length: Camera focal length (if None, estimates from FOV)\n",
        "        \"\"\"\n",
        "        self.image_width = image_width\n",
        "        self.image_height = image_height\n",
        "\n",
        "        # Estimate focal length if not provided (assuming 60Â° FOV)\n",
        "        if focal_length is None:\n",
        "            fov_deg = 60\n",
        "            self.focal_length = (image_width / 2) / np.tan(np.radians(fov_deg / 2))\n",
        "        else:\n",
        "            self.focal_length = focal_length\n",
        "\n",
        "        # Camera intrinsic matrix\n",
        "        self.K = np.array([\n",
        "            [self.focal_length, 0, image_width / 2],\n",
        "            [0, self.focal_length, image_height / 2],\n",
        "            [0, 0, 1]\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "    def compute_reprojection_error(self, object_points, image_points, R, tvec):\n",
        "        \"\"\"\n",
        "        Compute reprojection error (RMSE) for pose estimation validation\n",
        "\n",
        "        Args:\n",
        "            object_points: 3D points in world coordinates\n",
        "            image_points: 2D points in image coordinates\n",
        "            R: Rotation matrix\n",
        "            tvec: Translation vector\n",
        "\n",
        "        Returns:\n",
        "            rmse: Root Mean Square Error\n",
        "            reprojected_points: Reprojected 2D points\n",
        "            errors: Per-point errors\n",
        "        \"\"\"\n",
        "        # Convert rotation matrix to rotation vector\n",
        "        rvec, _ = cv2.Rodrigues(R)\n",
        "\n",
        "        # Project 3D points to 2D\n",
        "        reprojected_points, _ = cv2.projectPoints(\n",
        "            object_points, rvec, tvec.reshape(3, 1), self.K, None\n",
        "        )\n",
        "        reprojected_points = reprojected_points.reshape(-1, 2)\n",
        "\n",
        "        # Compute per-point errors\n",
        "        errors = np.linalg.norm(image_points - reprojected_points, axis=1)\n",
        "\n",
        "        # Compute RMSE\n",
        "        rmse = np.sqrt(np.mean(errors ** 2))\n",
        "\n",
        "        return rmse, reprojected_points, errors\n",
        "\n",
        "    def estimate_pose_from_plane(self, image_points, real_world_size=(0.3, 0.2)):\n",
        "        \"\"\"\n",
        "        Estimate camera pose from 4 corners of a planar rectangle\n",
        "\n",
        "        Args:\n",
        "            image_points: 4x2 array of corner points in image [top-left, top-right, bottom-right, bottom-left]\n",
        "            real_world_size: (width, height) of the plane in meters\n",
        "\n",
        "        Returns:\n",
        "            R: 3x3 rotation matrix (extrinsics)\n",
        "            t: 3x1 translation vector (extrinsics)\n",
        "            plane_normal: Normal vector of the plane\n",
        "            object_points: 3D world coordinates\n",
        "            rmse: Reprojection RMSE error\n",
        "            reprojected_points: Reprojected corner points\n",
        "        \"\"\"\n",
        "        # Define 3D points of the plane in world coordinates (on Z=0 plane)\n",
        "        width, height = real_world_size\n",
        "        object_points = np.array([\n",
        "            [0, 0, 0],           # top-left\n",
        "            [width, 0, 0],       # top-right\n",
        "            [width, height, 0],  # bottom-right\n",
        "            [0, height, 0]       # bottom-left\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Ensure image points are float32\n",
        "        image_points = np.array(image_points, dtype=np.float32)\n",
        "\n",
        "        # Use solvePnP to estimate pose\n",
        "        success, rvec, tvec = cv2.solvePnP(\n",
        "            object_points,\n",
        "            image_points,\n",
        "            self.K,\n",
        "            None,\n",
        "            flags=cv2.SOLVEPNP_ITERATIVE\n",
        "        )\n",
        "\n",
        "        if not success:\n",
        "            raise ValueError(\"Failed to estimate camera pose\")\n",
        "\n",
        "        # Convert rotation vector to rotation matrix\n",
        "        R_mat, _ = cv2.Rodrigues(rvec)\n",
        "\n",
        "        # The plane normal in camera coordinates is the third column of R\n",
        "        plane_normal = R_mat[:, 2]\n",
        "\n",
        "        # Compute reprojection error\n",
        "        rmse, reprojected_points, per_point_errors = self.compute_reprojection_error(\n",
        "            object_points, image_points, R_mat, tvec.flatten()\n",
        "        )\n",
        "\n",
        "        return R_mat, tvec.flatten(), plane_normal, object_points, rmse, reprojected_points\n",
        "\n",
        "    def get_camera_matrices_for_pytorch3d(self, R_mat, tvec):\n",
        "        \"\"\"\n",
        "        Convert OpenCV camera parameters to PyTorch3D format\n",
        "\n",
        "        Args:\n",
        "            R_mat: 3x3 rotation matrix (OpenCV convention)\n",
        "            tvec: 3x1 translation vector\n",
        "\n",
        "        Returns:\n",
        "            R_pytorch3d: Rotation matrix in PyTorch3D convention\n",
        "            T_pytorch3d: Translation vector in PyTorch3D convention\n",
        "        \"\"\"\n",
        "        # PyTorch3D uses a different coordinate system than OpenCV\n",
        "        # OpenCV: +X right, +Y down, +Z forward\n",
        "        # PyTorch3D: +X left, +Y up, +Z forward\n",
        "\n",
        "        # Conversion matrix\n",
        "        coord_transform = np.array([\n",
        "            [-1, 0, 0],\n",
        "            [0, -1, 0],\n",
        "            [0, 0, 1]\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        # Transform rotation and translation\n",
        "        R_pytorch3d = coord_transform @ R_mat\n",
        "        T_pytorch3d = coord_transform @ tvec\n",
        "\n",
        "        return R_pytorch3d, T_pytorch3d\n",
        "\n",
        "    def get_camera_parameters_text(self, R, t):\n",
        "        \"\"\"\n",
        "        Format camera parameters for display\n",
        "\n",
        "        Args:\n",
        "            R: Rotation matrix (extrinsics)\n",
        "            t: Translation vector (extrinsics)\n",
        "\n",
        "        Returns:\n",
        "            formatted_text: Human-readable camera parameters\n",
        "        \"\"\"\n",
        "        # Extract rotation angles (Euler angles)\n",
        "        from scipy.spatial.transform import Rotation as Rot\n",
        "        euler_angles = Rot.from_matrix(R).as_euler('xyz', degrees=True)\n",
        "\n",
        "        # Camera center in world coordinates\n",
        "        C = -R.T @ t\n",
        "\n",
        "        text = f\"\"\"\n",
        "ðŸ“· CAMERA INTRINSICS (K):\n",
        "  fx = {self.K[0,0]:.2f} px  (focal length X)\n",
        "  fy = {self.K[1,1]:.2f} px  (focal length Y)\n",
        "  cx = {self.K[0,2]:.2f} px  (principal point X)\n",
        "  cy = {self.K[1,2]:.2f} px  (principal point Y)\n",
        "\n",
        "  Matrix K:\n",
        "  [{self.K[0,0]:8.2f}  {self.K[0,1]:8.2f}  {self.K[0,2]:8.2f}]\n",
        "  [{self.K[1,0]:8.2f}  {self.K[1,1]:8.2f}  {self.K[1,2]:8.2f}]\n",
        "  [{self.K[2,0]:8.2f}  {self.K[2,1]:8.2f}  {self.K[2,2]:8.2f}]\n",
        "\n",
        "ðŸ“ CAMERA EXTRINSICS:\n",
        "\n",
        "  Rotation Matrix R:\n",
        "  [{R[0,0]:8.4f}  {R[0,1]:8.4f}  {R[0,2]:8.4f}]\n",
        "  [{R[1,0]:8.4f}  {R[1,1]:8.4f}  {R[1,2]:8.4f}]\n",
        "  [{R[2,0]:8.4f}  {R[2,1]:8.4f}  {R[2,2]:8.4f}]\n",
        "\n",
        "  Translation Vector t:\n",
        "  [{t[0]:8.4f}, {t[1]:8.4f}, {t[2]:8.4f}] meters\n",
        "\n",
        "  Euler Angles (X, Y, Z):\n",
        "  Roll  (X): {euler_angles[0]:7.2f}Â°\n",
        "  Pitch (Y): {euler_angles[1]:7.2f}Â°\n",
        "  Yaw   (Z): {euler_angles[2]:7.2f}Â°\n",
        "\n",
        "  Camera Center in World:\n",
        "  [{C[0]:8.4f}, {C[1]:8.4f}, {C[2]:8.4f}] meters\n",
        "\"\"\"\n",
        "        return text"
      ],
      "metadata": {
        "id": "cgw_-NlRXGGd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cube_mesh(device, size=0.15, color=[0.0, 0.5, 1.0]):\n",
        "    \"\"\"Create a cube mesh - INCREASED base size for better visibility\"\"\"\n",
        "    # Define cube vertices\n",
        "    vertices = torch.tensor([\n",
        "        [-1, -1, -1], [1, -1, -1], [1, 1, -1], [-1, 1, -1],  # back face\n",
        "        [-1, -1, 1], [1, -1, 1], [1, 1, 1], [-1, 1, 1]       # front face\n",
        "    ], dtype=torch.float32, device=device) * size / 2\n",
        "\n",
        "    # Define cube faces\n",
        "    faces = torch.tensor([\n",
        "        [0, 1, 2], [0, 2, 3],  # back\n",
        "        [4, 6, 5], [4, 7, 6],  # front\n",
        "        [0, 4, 5], [0, 5, 1],  # bottom\n",
        "        [2, 6, 7], [2, 7, 3],  # top\n",
        "        [0, 3, 7], [0, 7, 4],  # left\n",
        "        [1, 5, 6], [1, 6, 2]   # right\n",
        "    ], dtype=torch.int64, device=device)\n",
        "\n",
        "    # Create vertex colors\n",
        "    verts_rgb = torch.tensor([color] * vertices.shape[0], dtype=torch.float32, device=device)\n",
        "    textures = TexturesVertex(verts_features=[verts_rgb])\n",
        "\n",
        "    return Meshes(verts=[vertices], faces=[faces], textures=textures)\n",
        "\n",
        "\n",
        "def create_sphere_mesh(device, radius=0.08, color=[1.0, 0.0, 0.5]):\n",
        "    \"\"\"Create a sphere mesh - INCREASED base radius for better visibility\"\"\"\n",
        "    mesh = ico_sphere(3, device)\n",
        "\n",
        "    # Scale to desired radius\n",
        "    verts = mesh.verts_list()[0] * radius\n",
        "    faces = mesh.faces_list()[0]\n",
        "\n",
        "    # Create vertex colors\n",
        "    verts_rgb = torch.tensor([color] * verts.shape[0], dtype=torch.float32, device=device)\n",
        "    textures = TexturesVertex(verts_features=[verts_rgb])\n",
        "\n",
        "    return Meshes(verts=[verts], faces=[faces], textures=textures)\n",
        "\n",
        "\n",
        "def create_teapot_mesh(device, size=0.12, color=[0.0, 1.0, 0.5]):\n",
        "    \"\"\"Create a teapot-like shape - INCREASED base size for better visibility\"\"\"\n",
        "    # Create main body (sphere)\n",
        "    mesh = ico_sphere(3, device)\n",
        "    verts = mesh.verts_list()[0]\n",
        "\n",
        "    # Scale and deform to teapot shape\n",
        "    verts = verts * size\n",
        "    verts[:, 2] = verts[:, 2] * 0.7  # flatten slightly\n",
        "\n",
        "    faces = mesh.faces_list()[0]\n",
        "\n",
        "    # Create vertex colors\n",
        "    verts_rgb = torch.tensor([color] * verts.shape[0], dtype=torch.float32, device=device)\n",
        "    textures = TexturesVertex(verts_features=[verts_rgb])\n",
        "\n",
        "    return Meshes(verts=[verts], faces=[faces], textures=textures)\n",
        "\n",
        "\n",
        "def create_pyramid_mesh(device, size=0.15, color=[1.0, 1.0, 0.0]):\n",
        "    \"\"\"Create a pyramid mesh - INCREASED base size for better visibility\"\"\"\n",
        "    h = size\n",
        "    base = size * 0.8\n",
        "\n",
        "    vertices = torch.tensor([\n",
        "        [0, h, 0],                    # apex\n",
        "        [-base/2, 0, -base/2],        # base corners\n",
        "        [base/2, 0, -base/2],\n",
        "        [base/2, 0, base/2],\n",
        "        [-base/2, 0, base/2]\n",
        "    ], dtype=torch.float32, device=device)\n",
        "\n",
        "    faces = torch.tensor([\n",
        "        [0, 1, 2], [0, 2, 3], [0, 3, 4], [0, 4, 1],  # sides\n",
        "        [1, 3, 2], [1, 4, 3]  # base\n",
        "    ], dtype=torch.int64, device=device)\n",
        "\n",
        "    verts_rgb = torch.tensor([color] * vertices.shape[0], dtype=torch.float32, device=device)\n",
        "    textures = TexturesVertex(verts_features=[verts_rgb])\n",
        "\n",
        "    return Meshes(verts=[vertices], faces=[faces], textures=textures)\n"
      ],
      "metadata": {
        "id": "ecLk42ywXLpO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ARRenderer:\n",
        "    \"\"\"Augmented Reality Renderer using PyTorch3D\"\"\"\n",
        "\n",
        "    def __init__(self, image_width, image_height, focal_length, device):\n",
        "        self.device = device\n",
        "        self.image_width = image_width\n",
        "        self.image_height = image_height\n",
        "        self.focal_length = focal_length\n",
        "\n",
        "    def setup_renderer(self, R, T, light_intensity=0.8):\n",
        "        \"\"\"\n",
        "        Setup PyTorch3D renderer with camera parameters\n",
        "\n",
        "        Args:\n",
        "            R: 3x3 rotation matrix\n",
        "            T: 3x1 translation vector\n",
        "            light_intensity: Intensity of lighting (0-1)\n",
        "        \"\"\"\n",
        "        # Convert to PyTorch tensors\n",
        "        R_tensor = torch.tensor(R, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        T_tensor = torch.tensor(T, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "\n",
        "        # Calculate focal length in NDC coordinates\n",
        "        focal_ndc = 2.0 * self.focal_length / min(self.image_width, self.image_height)\n",
        "\n",
        "        # Create camera with intrinsics\n",
        "        cameras = PerspectiveCameras(\n",
        "            device=self.device,\n",
        "            R=R_tensor,\n",
        "            T=T_tensor,\n",
        "            focal_length=((focal_ndc, focal_ndc),),\n",
        "            principal_point=((0.0, 0.0),),\n",
        "            image_size=((self.image_height, self.image_width),),\n",
        "            in_ndc=True\n",
        "        )\n",
        "\n",
        "        # Setup rasterization\n",
        "        raster_settings = RasterizationSettings(\n",
        "            image_size=(self.image_height, self.image_width),\n",
        "            blur_radius=0.0,\n",
        "            faces_per_pixel=1,\n",
        "            bin_size=0\n",
        "        )\n",
        "\n",
        "        # Setup lighting\n",
        "        lights = PointLights(\n",
        "            device=self.device,\n",
        "            location=[[0.0, 1.0, -2.0]],\n",
        "            ambient_color=((light_intensity, light_intensity, light_intensity),),\n",
        "            diffuse_color=((0.6, 0.6, 0.6),),\n",
        "            specular_color=((0.3, 0.3, 0.3),)\n",
        "        )\n",
        "\n",
        "        # Create renderer\n",
        "        renderer = MeshRenderer(\n",
        "            rasterizer=MeshRasterizer(\n",
        "                cameras=cameras,\n",
        "                raster_settings=raster_settings\n",
        "            ),\n",
        "            shader=SoftPhongShader(\n",
        "                device=self.device,\n",
        "                cameras=cameras,\n",
        "                lights=lights,\n",
        "                blend_params=BlendParams(background_color=(0.0, 0.0, 0.0))\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return renderer, cameras\n",
        "\n",
        "    def render_object_on_plane(self, mesh, renderer, position, rotation_angles=(0, 0, 0)):\n",
        "        \"\"\"\n",
        "        Render 3D object at specified position on plane\n",
        "\n",
        "        Args:\n",
        "            mesh: PyTorch3D mesh object\n",
        "            renderer: PyTorch3D renderer\n",
        "            position: (x, y, z) position on plane\n",
        "            rotation_angles: (rx, ry, rz) rotation in degrees\n",
        "\n",
        "        Returns:\n",
        "            rendered_image: RGBA image with transparency\n",
        "        \"\"\"\n",
        "        # Apply transformations\n",
        "        verts = mesh.verts_list()[0].clone()\n",
        "\n",
        "        # Apply rotation\n",
        "        if any(rotation_angles):\n",
        "            rotation_matrix = torch.tensor(\n",
        "                R.from_euler('xyz', rotation_angles, degrees=True).as_matrix(),\n",
        "                dtype=torch.float32,\n",
        "                device=self.device\n",
        "            )\n",
        "            verts = torch.matmul(verts, rotation_matrix.T)\n",
        "\n",
        "        # Apply translation\n",
        "        position_tensor = torch.tensor(position, dtype=torch.float32, device=self.device)\n",
        "        verts = verts + position_tensor\n",
        "\n",
        "        # Create new mesh with transformed vertices\n",
        "        transformed_mesh = Meshes(\n",
        "            verts=[verts],\n",
        "            faces=mesh.faces_list(),\n",
        "            textures=mesh.textures\n",
        "        )\n",
        "\n",
        "        # Render\n",
        "        with torch.no_grad():\n",
        "            rendered = renderer(transformed_mesh)\n",
        "\n",
        "        return rendered[0, ..., :3].cpu().numpy()\n",
        "\n",
        "    def composite_with_background(self, background_image, rendered_image, alpha_threshold=0.01):\n",
        "        \"\"\"\n",
        "        Composite rendered object with background image\n",
        "\n",
        "        Args:\n",
        "            background_image: Background image (RGB)\n",
        "            rendered_image: Rendered object image (RGB)\n",
        "            alpha_threshold: Threshold for transparency\n",
        "\n",
        "        Returns:\n",
        "            composited_image: Final AR image\n",
        "        \"\"\"\n",
        "        # Convert to numpy if needed\n",
        "        if torch.is_tensor(background_image):\n",
        "            background_image = background_image.cpu().numpy()\n",
        "        if torch.is_tensor(rendered_image):\n",
        "            rendered_image = rendered_image.cpu().numpy()\n",
        "\n",
        "        # Normalize if needed\n",
        "        if background_image.max() > 1.0:\n",
        "            background_image = background_image / 255.0\n",
        "        if rendered_image.max() > 1.0:\n",
        "            rendered_image = rendered_image / 255.0\n",
        "\n",
        "        # Create alpha mask (where rendered object exists)\n",
        "        alpha = (rendered_image.sum(axis=2) > alpha_threshold).astype(np.float32)\n",
        "        alpha = np.expand_dims(alpha, axis=2)\n",
        "\n",
        "        # Composite\n",
        "        composited = background_image * (1 - alpha) + rendered_image * alpha\n",
        "\n",
        "        return composited"
      ],
      "metadata": {
        "id": "o60Y8Yu7XQH1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global variables for state management\n",
        "global_state = {\n",
        "    'image': None,\n",
        "    'corners': [],\n",
        "    'planes': [],\n",
        "    'pose_estimator': None,\n",
        "    'renderer': None,\n",
        "    'device': None,\n",
        "    'last_R': None,\n",
        "    'last_t': None,\n",
        "    'last_rmse': None,\n",
        "    'last_reprojected': None\n",
        "}\n",
        "\n",
        "def process_image_upload(image):\n",
        "    \"\"\"Process uploaded image\"\"\"\n",
        "    global global_state\n",
        "\n",
        "    if image is None:\n",
        "        return None, \"Please upload an image\", None\n",
        "\n",
        "    # Store image\n",
        "    global_state['image'] = np.array(image)\n",
        "    global_state['corners'] = []\n",
        "    global_state['planes'] = []\n",
        "\n",
        "    # Setup device\n",
        "    global_state['device'] = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Create pose estimator\n",
        "    h, w = global_state['image'].shape[:2]\n",
        "    global_state['pose_estimator'] = CameraPoseEstimator(w, h)\n",
        "\n",
        "    return image, f\"âœ“ Image loaded ({w}x{h}). Click on 4 corners to define plane.\", None\n",
        "\n",
        "\n",
        "def visualize_reprojection_error(image, corners, reprojected_points, rmse):\n",
        "    \"\"\"Create visualization of reprojection errors\"\"\"\n",
        "    vis_img = image.copy()\n",
        "\n",
        "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]\n",
        "    labels = ['TL', 'TR', 'BR', 'BL']\n",
        "\n",
        "    # Draw original corners (larger circles)\n",
        "    for i, (cx, cy) in enumerate(corners):\n",
        "        cv2.circle(vis_img, (int(cx), int(cy)), 10, colors[i], -1)\n",
        "        cv2.putText(vis_img, labels[i], (int(cx)+15, int(cy)-15),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, colors[i], 2)\n",
        "\n",
        "    # Draw reprojected corners (smaller circles with 'X')\n",
        "    for i, (cx, cy) in enumerate(reprojected_points):\n",
        "        cv2.circle(vis_img, (int(cx), int(cy)), 6, (255, 255, 255), 2)\n",
        "        cv2.drawMarker(vis_img, (int(cx), int(cy)), (0, 0, 0),\n",
        "                      cv2.MARKER_CROSS, 10, 2)\n",
        "\n",
        "    # Draw error lines\n",
        "    for i in range(len(corners)):\n",
        "        pt1 = (int(corners[i][0]), int(corners[i][1]))\n",
        "        pt2 = (int(reprojected_points[i][0]), int(reprojected_points[i][1]))\n",
        "        cv2.line(vis_img, pt1, pt2, (255, 0, 255), 2)\n",
        "\n",
        "        # Calculate and display individual error\n",
        "        error = np.linalg.norm(corners[i] - reprojected_points[i])\n",
        "        mid_x = int((corners[i][0] + reprojected_points[i][0]) / 2)\n",
        "        mid_y = int((corners[i][1] + reprojected_points[i][1]) / 2)\n",
        "        cv2.putText(vis_img, f\"{error:.1f}px\", (mid_x, mid_y),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 255), 2)\n",
        "\n",
        "    # Add RMSE text\n",
        "    cv2.putText(vis_img, f\"RMSE: {rmse:.2f} pixels\", (10, 30),\n",
        "               cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 3)\n",
        "\n",
        "    return vis_img\n",
        "\n",
        "\n",
        "def add_corner(image, evt: gr.SelectData):\n",
        "    \"\"\"Add corner point when user clicks on image\"\"\"\n",
        "    global global_state\n",
        "\n",
        "    if global_state['image'] is None:\n",
        "        return image, \"Please upload an image first\", None\n",
        "\n",
        "    x, y = evt.index\n",
        "    global_state['corners'].append([x, y])\n",
        "\n",
        "    # Draw corners on image\n",
        "    img_with_corners = global_state['image'].copy()\n",
        "\n",
        "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255), (255, 255, 0)]\n",
        "    labels = ['TL', 'TR', 'BR', 'BL']\n",
        "\n",
        "    for i, (cx, cy) in enumerate(global_state['corners']):\n",
        "        cv2.circle(img_with_corners, (cx, cy), 8, colors[i], -1)\n",
        "        cv2.putText(img_with_corners, labels[i], (cx+10, cy-10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, colors[i], 2)\n",
        "\n",
        "    # Draw lines between corners\n",
        "    if len(global_state['corners']) > 1:\n",
        "        for i in range(len(global_state['corners'])):\n",
        "            j = (i + 1) % len(global_state['corners'])\n",
        "            if j < len(global_state['corners']):\n",
        "                cv2.line(img_with_corners,\n",
        "                        tuple(global_state['corners'][i]),\n",
        "                        tuple(global_state['corners'][j]),\n",
        "                        (0, 255, 255), 2)\n",
        "\n",
        "    msg = f\"Corner {len(global_state['corners'])}/4 added\"\n",
        "\n",
        "    if len(global_state['corners']) == 4:\n",
        "        msg = \"âœ“ All 4 corners defined! Now you can render objects.\"\n",
        "        # Store this plane\n",
        "        global_state['planes'].append({\n",
        "            'corners': global_state['corners'].copy(),\n",
        "            'active': True\n",
        "        })\n",
        "\n",
        "    return img_with_corners, msg, None\n",
        "\n",
        "\n",
        "def reset_corners():\n",
        "    \"\"\"Reset corner selection\"\"\"\n",
        "    global global_state\n",
        "    global_state['corners'] = []\n",
        "\n",
        "    if global_state['image'] is not None:\n",
        "        return global_state['image'].copy(), \"Corners reset. Click 4 new corners.\", None, None\n",
        "    return None, \"Corners reset\", None, None\n",
        "\n",
        "\n",
        "def render_ar_scene(object_type, position_x, position_y, position_z,\n",
        "                   size_scale, rotation_x, rotation_y, rotation_z,\n",
        "                   light_intensity, plane_width, plane_height):\n",
        "    \"\"\"Main rendering function with enhanced feedback\"\"\"\n",
        "    global global_state\n",
        "\n",
        "    if global_state['image'] is None:\n",
        "        return None, \"Please upload an image first\", None, None\n",
        "\n",
        "    if len(global_state['corners']) != 4:\n",
        "        return global_state['image'], \"Please select 4 corners first\", None, None\n",
        "\n",
        "    try:\n",
        "        device = global_state['device']\n",
        "        pose_estimator = global_state['pose_estimator']\n",
        "\n",
        "        # Estimate camera pose from plane\n",
        "        corners = np.array(global_state['corners'], dtype=np.float32)\n",
        "        R_cv, t_cv, plane_normal, object_points, rmse, reprojected_points = pose_estimator.estimate_pose_from_plane(\n",
        "            corners,\n",
        "            real_world_size=(plane_width, plane_height)\n",
        "        )\n",
        "\n",
        "        # Store for visualization\n",
        "        global_state['last_R'] = R_cv\n",
        "        global_state['last_t'] = t_cv\n",
        "        global_state['last_rmse'] = rmse\n",
        "        global_state['last_reprojected'] = reprojected_points\n",
        "\n",
        "        # Convert to PyTorch3D format\n",
        "        R_pt3d, T_pt3d = pose_estimator.get_camera_matrices_for_pytorch3d(R_cv, t_cv)\n",
        "\n",
        "        # Setup renderer\n",
        "        ar_renderer = ARRenderer(\n",
        "            pose_estimator.image_width,\n",
        "            pose_estimator.image_height,\n",
        "            pose_estimator.focal_length,\n",
        "            device\n",
        "        )\n",
        "\n",
        "        renderer, cameras = ar_renderer.setup_renderer(R_pt3d, T_pt3d, light_intensity)\n",
        "\n",
        "        # Create 3D object based on selection\n",
        "        object_colors = {\n",
        "            'Cube': [0.0, 0.5, 1.0],\n",
        "            'Sphere': [1.0, 0.0, 0.5],\n",
        "            'Teapot': [0.0, 1.0, 0.5],\n",
        "            'Pyramid': [1.0, 1.0, 0.0]\n",
        "        }\n",
        "\n",
        "        color = object_colors.get(object_type, [0.5, 0.5, 0.5])\n",
        "\n",
        "        # FIXED: Use the new base sizes\n",
        "        if object_type == 'Cube':\n",
        "            mesh = create_cube_mesh(device, size=0.15 * size_scale, color=color)\n",
        "        elif object_type == 'Sphere':\n",
        "            mesh = create_sphere_mesh(device, radius=0.08 * size_scale, color=color)\n",
        "        elif object_type == 'Teapot':\n",
        "            mesh = create_teapot_mesh(device, size=0.12 * size_scale, color=color)\n",
        "        elif object_type == 'Pyramid':\n",
        "            mesh = create_pyramid_mesh(device, size=0.15 * size_scale, color=color)\n",
        "        else:\n",
        "            mesh = create_cube_mesh(device, size=0.15 * size_scale, color=color)\n",
        "\n",
        "        # Set object position on the plane\n",
        "        position = (position_x * plane_width, position_y * plane_height, position_z)\n",
        "\n",
        "        # Render object\n",
        "        rendered = ar_renderer.render_object_on_plane(\n",
        "            mesh, renderer, position,\n",
        "            rotation_angles=(rotation_x, rotation_y, rotation_z)\n",
        "        )\n",
        "\n",
        "        # Composite with background\n",
        "        background = global_state['image'] / 255.0\n",
        "        final_image = ar_renderer.composite_with_background(background, rendered)\n",
        "\n",
        "        # Convert to uint8 for display\n",
        "        final_image = (final_image * 255).astype(np.uint8)\n",
        "\n",
        "        # Generate camera parameters text\n",
        "        camera_params_text = pose_estimator.get_camera_parameters_text(R_cv, t_cv)\n",
        "\n",
        "        # Generate reprojection visualization\n",
        "        reproj_vis = visualize_reprojection_error(global_state['image'], corners, reprojected_points, rmse)\n",
        "\n",
        "        # Create detailed status message\n",
        "        status_msg = f\"\"\"âœ… {object_type} Rendered Successfully!\n",
        "\n",
        "ðŸ“Š REPROJECTION ERROR (RMSE): {rmse:.3f} pixels\n",
        "   {'âœ“ Excellent' if rmse < 1.0 else 'âœ“ Good' if rmse < 3.0 else 'âš  Fair' if rmse < 5.0 else 'âŒ Poor'} pose estimation quality\n",
        "\n",
        "{camera_params_text}\n",
        "\n",
        "ðŸ’¡ Tips:\n",
        "- Lower RMSE = better pose estimation\n",
        "- Reprojection visualization shows prediction accuracy\n",
        "- Adjust plane width/height if object appears misaligned\n",
        "\"\"\"\n",
        "\n",
        "        return final_image, status_msg, reproj_vis, camera_params_text\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_detail = traceback.format_exc()\n",
        "        return global_state['image'], f\"âŒ Error: {str(e)}\\n\\nDetails:\\n{error_detail}\", None, None\n",
        "\n",
        "\n",
        "def render_multiple_objects():\n",
        "    \"\"\"Render multiple objects at once\"\"\"\n",
        "    global global_state\n",
        "\n",
        "    if global_state['image'] is None:\n",
        "        return None, \"Please upload an image first\", None, None\n",
        "\n",
        "    if len(global_state['corners']) != 4:\n",
        "        return global_state['image'], \"Please select 4 corners first\", None, None\n",
        "\n",
        "    try:\n",
        "        device = global_state['device']\n",
        "        pose_estimator = global_state['pose_estimator']\n",
        "\n",
        "        # Estimate camera pose\n",
        "        corners = np.array(global_state['corners'], dtype=np.float32)\n",
        "        R_cv, t_cv, plane_normal, object_points, rmse, reprojected_points = pose_estimator.estimate_pose_from_plane(\n",
        "            corners, real_world_size=(0.3, 0.2)\n",
        "        )\n",
        "\n",
        "        # Store for visualization\n",
        "        global_state['last_R'] = R_cv\n",
        "        global_state['last_t'] = t_cv\n",
        "        global_state['last_rmse'] = rmse\n",
        "        global_state['last_reprojected'] = reprojected_points\n",
        "\n",
        "        R_pt3d, T_pt3d = pose_estimator.get_camera_matrices_for_pytorch3d(R_cv, t_cv)\n",
        "\n",
        "        # Setup renderer\n",
        "        ar_renderer = ARRenderer(\n",
        "            pose_estimator.image_width,\n",
        "            pose_estimator.image_height,\n",
        "            pose_estimator.focal_length,\n",
        "            device\n",
        "        )\n",
        "\n",
        "        renderer, cameras = ar_renderer.setup_renderer(R_pt3d, T_pt3d, 0.8)\n",
        "\n",
        "        # Create multiple objects with FIXED sizes\n",
        "        objects = [\n",
        "            ('Cube', (0.05, 0.05, 0.05), [0.0, 0.5, 1.0], 1.0),\n",
        "            ('Sphere', (0.15, 0.05, 0.05), [1.0, 0.0, 0.5], 0.8),\n",
        "            ('Teapot', (0.25, 0.05, 0.05), [0.0, 1.0, 0.5], 1.0),\n",
        "            ('Pyramid', (0.15, 0.15, 0.05), [1.0, 1.0, 0.0], 1.0)\n",
        "        ]\n",
        "\n",
        "        background = global_state['image'] / 255.0\n",
        "        result = background.copy()\n",
        "\n",
        "        for obj_type, position, color, scale in objects:\n",
        "            if obj_type == 'Cube':\n",
        "                mesh = create_cube_mesh(device, size=0.15 * scale, color=color)\n",
        "            elif obj_type == 'Sphere':\n",
        "                mesh = create_sphere_mesh(device, radius=0.08 * scale, color=color)\n",
        "            elif obj_type == 'Teapot':\n",
        "                mesh = create_teapot_mesh(device, size=0.12 * scale, color=color)\n",
        "            elif obj_type == 'Pyramid':\n",
        "                mesh = create_pyramid_mesh(device, size=0.15 * scale, color=color)\n",
        "\n",
        "            rendered = ar_renderer.render_object_on_plane(mesh, renderer, position)\n",
        "            result = ar_renderer.composite_with_background(result, rendered)\n",
        "\n",
        "        result = (result * 255).astype(np.uint8)\n",
        "\n",
        "        # Generate camera parameters text\n",
        "        camera_params_text = pose_estimator.get_camera_parameters_text(R_cv, t_cv)\n",
        "\n",
        "        # Generate reprojection visualization\n",
        "        reproj_vis = visualize_reprojection_error(global_state['image'], corners, reprojected_points, rmse)\n",
        "\n",
        "        status_msg = f\"\"\"âœ… Multiple Objects Rendered Successfully!\n",
        "\n",
        "ðŸ“Š REPROJECTION ERROR (RMSE): {rmse:.3f} pixels\n",
        "\n",
        "{camera_params_text}\n",
        "\"\"\"\n",
        "\n",
        "        return result, status_msg, reproj_vis, camera_params_text\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        error_detail = traceback.format_exc()\n",
        "        return global_state['image'], f\"âŒ Error: {str(e)}\\n\\nDetails:\\n{error_detail}\", None, None\n"
      ],
      "metadata": {
        "id": "koRaYYBUXUj9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_gradio_interface():\n",
        "    \"\"\"Create the Gradio interface with enhanced metrics display\"\"\"\n",
        "\n",
        "    with gr.Blocks(title=\"AR with PyTorch3D - Enhanced\", theme=gr.themes.Soft()) as demo:\n",
        "        gr.Markdown(\"\"\"\n",
        "        # ðŸŽ¯ Augmented Reality with PyTorch3D (Enhanced Edition)\n",
        "        ## Assignment 4: Render 3D objects with Camera Calibration Analysis\n",
        "\n",
        "        ### Instructions:\n",
        "        1. **Upload** an image of a book/object on a desk\n",
        "        2. **Click 4 corners** in order: Top-Left â†’ Top-Right â†’ Bottom-Right â†’ Bottom-Left\n",
        "        3. **Adjust parameters** and render your AR scene!\n",
        "        4. **View** camera intrinsics, extrinsics, and RMSE reprojection error\n",
        "\n",
        "        ðŸ“¸ **Recommended Image**: Take a photo of a book or laptop on a table at a slight angle.\n",
        "        \"\"\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                # Image upload and display\n",
        "                input_image = gr.Image(label=\"ðŸ“¤ Upload Image\", type=\"numpy\")\n",
        "                image_display = gr.Image(label=\"ðŸ–±ï¸ Click to Add Corners (4 required)\", type=\"numpy\", interactive=False)\n",
        "                status_text = gr.Textbox(label=\"Status\", value=\"Upload an image to start\", lines=2)\n",
        "\n",
        "                with gr.Row():\n",
        "                    reset_btn = gr.Button(\"ðŸ”„ Reset Corners\", variant=\"secondary\")\n",
        "\n",
        "                # Add reprojection visualization\n",
        "                reproj_image = gr.Image(label=\"ðŸ“ Reprojection Error Visualization\", type=\"numpy\")\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                gr.Markdown(\"### ðŸŽ® Object Controls\")\n",
        "\n",
        "                object_type = gr.Dropdown(\n",
        "                    choices=['Cube', 'Sphere', 'Teapot', 'Pyramid'],\n",
        "                    value='Cube',\n",
        "                    label=\"Object Type\"\n",
        "                )\n",
        "\n",
        "                with gr.Accordion(\"ðŸ“ Position\", open=True):\n",
        "                    position_x = gr.Slider(0, 1, value=0.5, step=0.05, label=\"X Position (0=left, 1=right)\")\n",
        "                    position_y = gr.Slider(0, 1, value=0.5, step=0.05, label=\"Y Position (0=top, 1=bottom)\")\n",
        "                    position_z = gr.Slider(-0.1, 0.3, value=0.08, step=0.01, label=\"Z Height (increased default for visibility)\")\n",
        "\n",
        "                with gr.Accordion(\"ðŸ”„ Rotation\", open=False):\n",
        "                    rotation_x = gr.Slider(-180, 180, value=0, step=5, label=\"Rotation X\")\n",
        "                    rotation_y = gr.Slider(-180, 180, value=0, step=5, label=\"Rotation Y\")\n",
        "                    rotation_z = gr.Slider(-180, 180, value=0, step=5, label=\"Rotation Z\")\n",
        "\n",
        "                with gr.Accordion(\"âš™ï¸ Advanced\", open=False):\n",
        "                    size_scale = gr.Slider(0.5, 3.0, value=1.0, step=0.1, label=\"Object Size\")\n",
        "                    light_intensity = gr.Slider(0.3, 1.0, value=0.8, step=0.05, label=\"Light Intensity\")\n",
        "                    plane_width = gr.Slider(0.1, 0.5, value=0.3, step=0.05, label=\"Plane Width (m)\")\n",
        "                    plane_height = gr.Slider(0.1, 0.5, value=0.2, step=0.05, label=\"Plane Height (m)\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    render_btn = gr.Button(\"ðŸŽ¨ Render Single Object\", variant=\"primary\", size=\"lg\")\n",
        "                    render_multi_btn = gr.Button(\"âœ¨ Render Multiple Objects\", variant=\"secondary\")\n",
        "\n",
        "                output_image = gr.Image(label=\"ðŸ–¼ï¸ AR Output\")\n",
        "                output_status = gr.Textbox(label=\"ðŸ“Š Detailed Status & Camera Parameters\", lines=15)\n",
        "\n",
        "        # Camera parameters display (moved to bottom for better visibility)\n",
        "        with gr.Row():\n",
        "            camera_params_display = gr.Textbox(\n",
        "                label=\"ðŸ“· Camera Intrinsics & Extrinsics (Full Details)\",\n",
        "                lines=25,\n",
        "                max_lines=30\n",
        "            )\n",
        "\n",
        "        # Event handlers - updated to handle new outputs\n",
        "        input_image.upload(\n",
        "            process_image_upload,\n",
        "            inputs=[input_image],\n",
        "            outputs=[image_display, status_text, reproj_image]\n",
        "        )\n",
        "\n",
        "        image_display.select(\n",
        "            add_corner,\n",
        "            inputs=[image_display],\n",
        "            outputs=[image_display, status_text, reproj_image]\n",
        "        )\n",
        "\n",
        "        reset_btn.click(\n",
        "            reset_corners,\n",
        "            outputs=[image_display, status_text, reproj_image, camera_params_display]\n",
        "        )\n",
        "\n",
        "        render_btn.click(\n",
        "            render_ar_scene,\n",
        "            inputs=[object_type, position_x, position_y, position_z,\n",
        "                   size_scale, rotation_x, rotation_y, rotation_z,\n",
        "                   light_intensity, plane_width, plane_height],\n",
        "            outputs=[output_image, output_status, reproj_image, camera_params_display]\n",
        "        )\n",
        "\n",
        "        render_multi_btn.click(\n",
        "            render_multiple_objects,\n",
        "            outputs=[output_image, output_status, reproj_image, camera_params_display]\n",
        "        )\n",
        "\n",
        "    return demo\n"
      ],
      "metadata": {
        "id": "RsRNRKdrXaS9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_example_visualization():\n",
        "    \"\"\"Create example visualizations for documentation\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXAMPLE: Creating synthetic test case\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Create a synthetic test image\n",
        "    img_width, img_height = 640, 480\n",
        "    test_image = np.ones((img_height, img_width, 3), dtype=np.uint8) * 240\n",
        "\n",
        "    # Draw a rectangle to simulate a book\n",
        "    rect_corners = np.array([\n",
        "        [150, 120],  # top-left\n",
        "        [490, 140],  # top-right\n",
        "        [470, 360],  # bottom-right\n",
        "        [130, 340]   # bottom-left\n",
        "    ], dtype=np.int32)\n",
        "\n",
        "    cv2.fillPoly(test_image, [rect_corners], (200, 180, 160))\n",
        "    cv2.polylines(test_image, [rect_corners], True, (100, 80, 60), 3)\n",
        "\n",
        "    # Add some texture\n",
        "    for i in range(10):\n",
        "        y = 150 + i * 20\n",
        "        cv2.line(test_image, (160, y), (460, y), (180, 160, 140), 1)\n",
        "\n",
        "    # Save example\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.imshow(test_image)\n",
        "    plt.title(\"Example Input Image with Book\")\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('example_input.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"âœ“ Example input image saved as 'example_input.png'\")\n",
        "\n",
        "    return test_image, rect_corners\n",
        "\n",
        "\n",
        "def run_example_pipeline():\n",
        "    \"\"\"Run a complete example pipeline\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"RUNNING EXAMPLE PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Create test image\n",
        "    test_image, corners = create_example_visualization()\n",
        "    h, w = test_image.shape[:2]\n",
        "\n",
        "    # Step 1: Camera Pose Estimation\n",
        "    print(\"\\n1. Estimating camera pose...\")\n",
        "    pose_estimator = CameraPoseEstimator(w, h)\n",
        "    R_cv, t_cv, plane_normal, object_points = pose_estimator.estimate_pose_from_plane(\n",
        "        corners, real_world_size=(0.3, 0.2)\n",
        "    )\n",
        "    print(f\"   âœ“ Rotation matrix:\\n{R_cv}\")\n",
        "    print(f\"   âœ“ Translation: {t_cv}\")\n",
        "\n",
        "    # Step 2: Convert to PyTorch3D\n",
        "    print(\"\\n2. Converting to PyTorch3D format...\")\n",
        "    R_pt3d, T_pt3d = pose_estimator.get_camera_matrices_for_pytorch3d(R_cv, t_cv)\n",
        "    print(f\"   âœ“ PyTorch3D rotation:\\n{R_pt3d}\")\n",
        "    print(f\"   âœ“ PyTorch3D translation: {T_pt3d}\")\n",
        "\n",
        "    # Step 3: Setup Renderer\n",
        "    print(\"\\n3. Setting up renderer...\")\n",
        "    ar_renderer = ARRenderer(w, h, pose_estimator.focal_length, device)\n",
        "    renderer, cameras = ar_renderer.setup_renderer(R_pt3d, T_pt3d, 0.8)\n",
        "    print(\"   âœ“ Renderer configured\")\n",
        "\n",
        "    # Step 4: Create and render objects\n",
        "    print(\"\\n4. Creating and rendering 3D objects...\")\n",
        "\n",
        "    objects_to_render = [\n",
        "        ('Cube', (0.08, 0.05, 0.05), create_cube_mesh(device, 0.08, [0.0, 0.5, 1.0])),\n",
        "        ('Sphere', (0.15, 0.05, 0.05), create_sphere_mesh(device, 0.04, [1.0, 0.0, 0.5])),\n",
        "        ('Teapot', (0.22, 0.05, 0.05), create_teapot_mesh(device, 0.08, [0.0, 1.0, 0.5]))\n",
        "    ]\n",
        "\n",
        "    background = test_image / 255.0\n",
        "    result = background.copy()\n",
        "\n",
        "    for name, position, mesh in objects_to_render:\n",
        "        print(f\"   - Rendering {name}...\")\n",
        "        rendered = ar_renderer.render_object_on_plane(mesh, renderer, position)\n",
        "        result = ar_renderer.composite_with_background(result, rendered)\n",
        "\n",
        "    result = (result * 255).astype(np.uint8)\n",
        "\n",
        "    # Save result\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(test_image)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(result)\n",
        "    plt.title(\"AR Result with Multiple Objects\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('example_ar_output.png', dpi=150, bbox_inches='tight')\n",
        "    print(\"\\nâœ“ Example AR output saved as 'example_ar_output.png'\")\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"EXAMPLE PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "    print(\"=\"*60)"
      ],
      "metadata": {
        "id": "Blfv0_4GXezd"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" \"*15 + \"AR WITH PYTORCH3D - ASSIGNMENT 4\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Check CUDA availability\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"âœ“ CUDA Available: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"âš  Running on CPU (CUDA not available)\")\n",
        "\n",
        "    # Run example pipeline\n",
        "    print(\"\\nRunning example pipeline to verify setup...\")\n",
        "    try:\n",
        "        run_example_pipeline()\n",
        "        print(\"\\nâœ“ Example pipeline successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâš  Example pipeline error (this is OK): {e}\")\n",
        "\n",
        "    # Launch Gradio interface\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"LAUNCHING GRADIO INTERFACE...\")\n",
        "    print(\"=\"*70)\n",
        "    print(\"\\nðŸ“ Instructions:\")\n",
        "    print(\"1. The interface will open in a new browser tab\")\n",
        "    print(\"2. Upload your image of a book/object on a desk\")\n",
        "    print(\"3. Click 4 corners: Top-Left â†’ Top-Right â†’ Bottom-Right â†’ Bottom-Left\")\n",
        "    print(\"4. Adjust parameters and click 'Render'\")\n",
        "    print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
        "\n",
        "    # Create and launch interface\n",
        "    demo = create_gradio_interface()\n",
        "    demo.launch(\n",
        "        share=True,  # Creates public link\n",
        "        debug=False,\n",
        "        show_error=True\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9rr_Q2WHXiux",
        "outputId": "0bfc161a-cd2b-4098-df74-3be663bd8dc0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "               AR WITH PYTORCH3D - ASSIGNMENT 4\n",
            "======================================================================\n",
            "âš  Running on CPU (CUDA not available)\n",
            "\n",
            "Running example pipeline to verify setup...\n",
            "\n",
            "============================================================\n",
            "RUNNING EXAMPLE PIPELINE\n",
            "============================================================\n",
            "Using device: cpu\n",
            "\n",
            "============================================================\n",
            "EXAMPLE: Creating synthetic test case\n",
            "============================================================\n",
            "âœ“ Example input image saved as 'example_input.png'\n",
            "\n",
            "1. Estimating camera pose...\n",
            "\n",
            "âš  Example pipeline error (this is OK): too many values to unpack (expected 4)\n",
            "\n",
            "======================================================================\n",
            "LAUNCHING GRADIO INTERFACE...\n",
            "======================================================================\n",
            "\n",
            "ðŸ“ Instructions:\n",
            "1. The interface will open in a new browser tab\n",
            "2. Upload your image of a book/object on a desk\n",
            "3. Click 4 corners: Top-Left â†’ Top-Right â†’ Bottom-Right â†’ Bottom-Left\n",
            "4. Adjust parameters and click 'Render'\n",
            "\n",
            "======================================================================\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://bd8960a95acffa9625.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://bd8960a95acffa9625.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA3MAAAKyCAYAAAB/ioffAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAOihJREFUeJzt3XecXnWd6PHvMzU9kw4JqcQQCFUIaJZICySAsOKliJcVVBQR4WJB4boKLCpXVKzAriJlXUFUWJemEQExFKnSW0ISkJKembTJtOfcP9jMMswEUubMk9/M+/16oeacM9/nNzPEzCfnPOcUsizLAgAAgKSUlXoBAAAAbD4xBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxB5CoP//5z1EoFOLPf/5zqZfCNm7hwoVRKBTimmuu2eRjv/vd7+a/sBwceOCBseuuu5Z6GQBdQswB3dI111wThUJho//89a9/LfUStznb4g/xl19++SYFyAaFQiE+97nP5begbuT222+PCy64oNPnbvhLhrf+M3jw4Hjf+94Xv/zlLzv99QB6sopSLwAgT//yL/8S48ePb7d94sSJJVgNm+vyyy+PoUOHximnnFLqpSRt7NixUV9fH5WVla3bbr/99rjssstyCbqIiLPOOiumTp0aERHLly+PG264IU466aSora2NM844I5fXBOhpxBzQrR1++OGxzz77lHoZUFKFQiF69erVpa85ffr0OPbYY1t/ffrpp8eECRPiuuuuE3MAncRllkCPdv7550dZWVnceeedbbZ/+tOfjqqqqnjiiSciIqKxsTG+/vWvx9577x0DBw6Mvn37xvTp0+Puu+9u83FvvVTxsssuiwkTJkSfPn3isMMOi7///e+RZVlcdNFFscMOO0Tv3r3jH//xH2PFihVtZowbNy4++MEPxh//+MfYc889o1evXrHLLrvETTfdtEmf04MPPhizZs2KgQMHRp8+feKAAw6I++67b4u+PhsuV73vvvviC1/4QgwbNiz69u0bxxxzTCxdunSL1n3BBRdEoVDY6GstXLiwdd4zzzwT99xzT+vlegceeOBmrX/DJX+//vWv48ILL4xRo0ZF//7949hjj426urpoaGiIs88+O4YPHx79+vWLj3/849HQ0NBmxtVXXx0HH3xwDB8+PKqrq2OXXXaJK664ot1rFYvFuOCCC2LkyJHRp0+fOOigg+LZZ5+NcePGtTuzWFtbG2effXaMHj06qqurY+LEifHtb387isXiO34+X/jCF2LIkCGRZVnrtjPPPDMKhUL86Ec/at22ePHiKBQKret8+3vmTjnllLjssssiItpcDvl2P/3pT2PHHXeM6urqmDp1ajz88MPvuL53UlVVFYMGDYqKirZ/j9zc3BwXXXRR6+uMGzcu/u///b/tvg8Rb56pnTJlSlRXV8fIkSPjjDPOiNra2nd97T/+8Y/Rp0+fOPHEE6O5uXmLPweAbY0zc0C3VldXF8uWLWuzrVAoxJAhQyIi4p//+Z/jlltuiU9+8pPx1FNPRf/+/WP27Nnxs5/9LC666KLYY489IiJi1apVceWVV8aJJ54Yn/rUp2L16tXx85//PGbOnBkPPfRQ7Lnnnm1e45e//GU0NjbGmWeeGStWrIhLLrkkjj/++Dj44IPjz3/+c3zlK1+JefPmxY9//OP40pe+FFdddVWbj587d26ccMIJ8ZnPfCZOPvnkuPrqq+O4446LP/zhD3HooYdu9PO966674vDDD4+99967NVQ3xMicOXNi33333aKv45lnnhmDBg2K888/PxYuXBg/+MEP4nOf+1zccMMNnbLujvzgBz+IM888M/r16xdf/epXIyJixIgRW7T+iy++OHr37h3nnntu69e9srIyysrKYuXKlXHBBRfEX//617jmmmti/Pjx8fWvf731Y6+44oqYMmVKHH300VFRURG33HJLfPazn41isdjmDNN5550Xl1xySRx11FExc+bMeOKJJ2LmzJmxfv36NmtZt25dHHDAAfHaa6/FaaedFmPGjIn7778/zjvvvHjjjTfiBz/4wUY/j+nTp8f3v//9eOaZZ1pv8jFnzpwoKyuLOXPmxFlnndW6LSLiAx/4QIdzTjvttHj99dfjjjvuiF/84hcdHnPdddfF6tWr47TTTotCoRCXXHJJfPjDH4758+e3uVxzY1avXt36e2/FihVx3XXXxdNPPx0///nP2xx36qmnxrXXXhvHHntsfPGLX4wHH3wwLr744njuuefiP//zP1uPu+CCC+LCCy+MGTNmxOmnnx4vvPBCXHHFFfHwww/Hfffdt9E13XrrrXHsscfGCSecEFdddVWUl5e/69oBkpEBdENXX311FhEd/lNdXd3m2KeeeiqrqqrKTj311GzlypXZqFGjsn322SdrampqPaa5uTlraGho83ErV67MRowYkX3iE59o3bZgwYIsIrJhw4ZltbW1rdvPO++8LCKyPfbYo83cE088MauqqsrWr1/fum3s2LFZRGQ33nhj67a6urps++23z/baa6/WbXfffXcWEdndd9+dZVmWFYvF7D3veU82c+bMrFgsth63bt26bPz48dmhhx76jl+zDWv/zne+0+7rOGPGjDYzP//5z2fl5eVtPsdNXff555+fdfTHz4bXWrBgQeu2KVOmZAcccMA7rvutIiI744wzWn+94Wu06667Zo2Nja3bTzzxxKxQKGSHH354m49///vfn40dO7bNtnXr1rV7nZkzZ2YTJkxo/fWiRYuyioqK7EMf+lCb4y644IIsIrKTTz65ddtFF12U9e3bN3vxxRfbHHvuuedm5eXl2SuvvLLRz2/JkiVZRGSXX355lmVZVltbm5WVlWXHHXdcNmLEiNbjzjrrrGzw4MGt37MN39urr7669Zgzzjijw+/DhmOHDBmSrVixonX7f/3Xf2URkd1yyy0bXV+W/c/X/O3/lJWVZd/85jfbHPv4449nEZGdeuqpbbZ/6UtfyiIiu+uuu1o/76qqquywww7LWlpaWo/7yU9+kkVEdtVVV7VuO+CAA7IpU6ZkWZZlN954Y1ZZWZl96lOfavNxAN2FyyyBbu2yyy6LO+64o80/v//979scs+uuu8aFF14YV155ZcycOTOWLVsW1157bZvLwcrLy6Oqqioi3rycbsWKFdHc3Bz77LNPPPbYY+1e97jjjouBAwe2/nq//faLiIiTTjqpzdz99tsvGhsb47XXXmvz8SNHjoxjjjmm9dcDBgyIj33sY/G3v/0tFi1a1OHn+vjjj8fcuXPjox/9aCxfvjyWLVsWy5Yti7Vr18YhhxwSf/nLX971Mr6N+fSnP93mMrzp06dHS0tLvPzyy1u97q7wsY99rM2Zm/322y+yLItPfOITbY7bb7/94u9//3ubS/F69+7d+r83nOk94IADYv78+VFXVxcREXfeeWc0NzfHZz/72TbzzjzzzHZr+c1vfhPTp0+PQYMGtX6Pli1bFjNmzIiWlpb4y1/+stHPY9iwYTF58uTWY+67774oLy+Pc845JxYvXhxz586NiDfPzO2///4dXjq5qU444YQYNGhQ66+nT58eERHz58/fpI//+te/3vp77oYbbogTTzwxvvrVr8YPf/jD1mNuv/32iHjz8tG3+uIXvxgREbfddltERPzpT3+KxsbGOPvss6Os7H9+dPnUpz4VAwYMaD3ura6//vo44YQT4rTTTot/+7d/a/NxAN2FyyyBbm3ffffdpBugnHPOOfGrX/0qHnroofjWt74Vu+yyS7tjrr322vje974Xzz//fDQ1NbVu7+humWPGjGnz6w1hN3r06A63r1y5ss32iRMntvtBfNKkSRHx5vuftttuu3avueEH+ZNPPrnjTzLejJG3/oC+qd7++WyY0Rnr7gqb8/0oFotRV1fXeinufffdF+eff3488MADsW7dujbH19XVxcCBA1uj9u13SR08eHC7r/fcuXPjySefjGHDhnW41iVLlrzj5zJ9+vTWCJozZ07ss88+sc8++8TgwYNjzpw5MWLEiHjiiSfiox/96DvOeTeb+j3fmN122y1mzJjR+uvjjz8+6urq4txzz42PfvSjMWzYsHj55ZejrKys3ddtu+22i5qamtav64b/3mmnndocV1VVFRMmTGj3lwoLFiyIk046KY477rj48Y9/vEnrBUiRmAOIN882bIihp556qt3+//iP/4hTTjklPvShD8U555wTw4cPj/Ly8rj44ovjpZdeanf8xt6Xs7Ht2VtuaLGlNpx1+853vtPuPXwb9OvXb4tmd+a6N3a2qKWlZbNnbaot/X689NJLccghh8TkyZPj0ksvjdGjR0dVVVXcfvvt8f3vf3+LznQWi8U49NBD48tf/nKH+zfE78bsv//+8bOf/Szmz58fc+bMienTp0ehUIj9998/5syZEyNHjoxisdh6Jm1L5fHv6iGHHBK33nprPPTQQ3HkkUe2bt+aM4gd2X777WP77beP22+/PR555BF3tAW6LTEH9HjFYjFOOeWUGDBgQJx99tnxrW99K4499tj48Ic/3HrMb3/725gwYULcdNNNbX7wPP/883NZ07x58yLLsjav9eKLL0bEm3d57MiOO+4YEW9e2vjWMyJdaVPWveEMT21tbdTU1LQe9/azKxGd/0P+5rrllluioaEhbr755jZnqt5+F9OxY8dGxJuf/1vP1C5fvrzdmawdd9wx1qxZs8Xfow2Rdscdd8TDDz8c5557bkS8ebOTK664IkaOHBl9+/aNvffe+x3nlOJru+Hy1TVr1kTEm1+3YrEYc+fOjZ133rn1uMWLF0dtbW3r13XDf7/wwgsxYcKE1uMaGxtjwYIF7b6WvXr1iltvvTUOPvjgmDVrVtxzzz0xZcqUXD83gFJwATnQ41166aVx//33x09/+tO46KKLYtq0aXH66ae3uQvmhrMUbz0r8eCDD8YDDzyQy5pef/31NnfyW7VqVfz7v/977Lnnnhu9VHHvvfeOHXfcMb773e+2/rD8Vm9/lEAeNmXdG6Lzre8NW7t2bVx77bXt5vXt23eTbj2fl46+73V1dXH11Ve3Oe6QQw6JioqKdo8s+MlPftJu5vHHHx8PPPBAzJ49u92+2trad711/vjx42PUqFHx/e9/P5qamuIf/uEfIuLNyHvppZfit7/9bbzvfe9r9wiAt+vbt2/ra3aVW2+9NSKi9S6xRxxxREREuzt4XnrppRERrWfvZsyYEVVVVfGjH/2ozffi5z//edTV1bU5y7fBwIEDY/bs2TF8+PA49NBDOzyDDpA6Z+aAbu33v/99PP/88+22T5s2LSZMmBDPPfdcfO1rX4tTTjkljjrqqIh483lne+65Z3z2s5+NX//61xER8cEPfjBuuummOOaYY+LII4+MBQsWxL/+67/GLrvs0mE4ba1JkybFJz/5yXj44YdjxIgRcdVVV8XixYvbRcRblZWVxZVXXhmHH354TJkyJT7+8Y/HqFGj4rXXXou77747BgwYELfcckunr3Vz133YYYfFmDFj4pOf/GScc845UV5eHldddVUMGzYsXnnllTbz9t5777jiiiviG9/4RkycODGGDx8eBx98cK6fw1sddthhUVVVFUcddVScdtppsWbNmvjZz34Ww4cPjzfeeKP1uBEjRsT/+T//J773ve/F0UcfHbNmzYonnngifv/738fQoUPbnAU755xz4uabb44PfvCDccopp8Tee+8da9eujaeeeip++9vfxsKFC2Po0KHvuK7p06fHr371q9htt91az3S+973vjb59+8aLL764Se+X23Dm7qyzzoqZM2dGeXl5fOQjH9mSL1OH5syZ0/pYhhUrVsTNN98c99xzT3zkIx+JyZMnR8SbUXfyySfHT3/606itrY0DDjggHnroobj22mvjQx/6UBx00EER8eaNX84777y48MILY9asWXH00UfHCy+8EJdffnlMnTo1TjrppA7XMHTo0Ljjjjti//33jxkzZsS9994bo0aN6rTPEaDkSnUbTYA8vdOjCeK/b9He3NycTZ06Ndthhx3a3GI/y7Lshz/8YRYR2Q033JBl2Zu3/f/Wt76VjR07Nquurs722muv7NZbb81OPvnkNrey7+j2/ln2P7dr/81vftPhOh9++OHWbWPHjs2OPPLIbPbs2dnuu++eVVdXZ5MnT273sW9/NMEGf/vb37IPf/jD2ZAhQ7Lq6ups7Nix2fHHH5/deeed7/g1e6dHE7x1fRt77U1dd5Zl2aOPPprtt99+WVVVVTZmzJjs0ksv7fDRBIsWLcqOPPLIrH///llEvOtjCmIjjybYlK97lv3PYxOWLl3auu3mm2/Odt9996xXr17ZuHHjsm9/+9vZVVdd1W6tzc3N2de+9rVsu+22y3r37p0dfPDB2XPPPZcNGTIk+8xnPtPmdVavXp2dd9552cSJE7Oqqqps6NCh2bRp07Lvfve7bR6hsDGXXXZZFhHZ6aef3mb7jBkzsoho973u6NEEzc3N2ZlnnpkNGzYsKxQKrY8p2Ni/w1n25tf3/PPPf8e1dfRogqqqqmzy5MnZN7/5zXafX1NTU3bhhRdm48ePzyorK7PRo0dn5513XpvHdWzwk5/8JJs8eXJWWVmZjRgxIjv99NOzlStXtjnmrY8m2GDevHnZ9ttvn+28885tvrcAqStkWSe86x6ATjNu3LjYddddWy9JS0Wq685TbW1tDBo0KL7xjW+0PvgcADqL98wBQCeor69vt23De8EOPPDArl0MAD2C98wBQCe44YYb4pprrokjjjgi+vXrF/fee29cf/31cdhhh7XepAQAOpOYA4BOsPvuu0dFRUVccsklsWrVqtabonzjG98o9dIA6Ka8Zw4AACBB3jMHAACQIDEHAACQIDEHAACQoE2+AUpdXV2e6wAAAOC/DRw48F2PcWYOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQWIOAAAgQRWlXgB0J4/ce2c8+sDdm3z8jjvtGjOO/kiOKwIAoLsSc9AJsiyL5558JJ55/MGY+8zjm/xxjevXx9DtRuWypiHDRsTo8ZNymQ0AQOkVsizLNuXAurq6vNcCSSoWi9HU2Bjf/epnY+XyJaVeTqt9/uGQOOZjp+cyu7y8PCoqKnOZDQBAxMCBA9/1GDEHW+mNvy+MH130hWhqbCz1UtooKyuL8op8Tr5PP/ToOOK4U3KZDQDApsWcyyxhK2VZts2FXMSbZwyLOa3ryUfuj6WLX89l9p77fiD22Hf/XGYDAHQnYg7YbMsWvx7Lcoq5yqrqqO7dO5fZw7cfHYOHDs9lNgBAVxNzkKNCoRCFQmGj+7Msi0280rnHeOz+u+Ox+zf9jqCb46iPfDL+YcYHc5ldVlYeZWWe9gIAdB0xBznaeeedY++9997o/oULF8acOXO6cEU92x9u/EX86eZf5TL7mH86Pd77/gNzmQ0A0BExBzkqLy+P6urqje4fOXJkfOADH8jltd94442YO3duLrNT1dTUGE1N+byP8P67bo8XN+OxFJtj/xkfjB3GTcxlNgCQLjEHJdS/f//o379/LrN79eoVq1atymX22rVrY82aNbnMTtXCuc/GwrnP5jJ72Ijtc7vJznajxkTvvv1ymQ0A5EvMwVZ48z1vxVIvo0NjxoyJMWPG5DL7qaeeigcffDCX2bT3+xt/kdvsT33xX+I9U/bMZfa7vWcUANg6njMHW+EPN/4i7r/79li3ZnW7fUcccUQMHTo0qqqqSrCyfDU1NUVTU1Mus5988sl4+umnc5lNe7379svtAfCf/Pz5Lg8FgC3kOXOQs/Xr13UYchFvXubYHUMuIqKysjIqK/MJgPHjx0e/fvlc9rdw4cJYtGhRLrNTVb82v8tl/3z7jdG/ZlAusw864tgYUDM4l9kAkAoxB2xTRowYESNGjMhldktLSzTm9N6zNWvW5DY7VY8/lN+dWsdPmhJDh4/MZfbwkTvkdrYSADqTmAN6jD322CP22GOPXGbfc8897h7ahf79JxfnNvu871wZg4fm8xcKEeF9hAB0GjEH0An23Xff2GuvvXKZfd9998Vrr72Wy2zau+Lic6OsvPP/eCwUCnHmP383+g149/dAAMCmEHMAnaB3797Ru3fvXGbvtNNOuV16Onfu3Fi9uuP3ffZUtSuW5Tb7rtt+E9W9enX63EKhPA464n9FZTd9ny4AHRNzANu4CRMm5DZ79erVuV32t3bt2mhpaclldqr+Mvt3ucwtKy+PXfacGr169+n84YVCDBm2nctDAbZBYg6gB/vABz6Q2+ybb745li5dmtt8/kexpSV+cMHZucyuqqqOC358XVRVV+cyH4AtJ+YAerA8z7YcdNBB0dzcnMvsP/3pT7Fq1apcZtNWU1Nj/PDCz+fy70qvPn3i9HP/X5SXl3f6bICeQMwBkIsBAwbkNnvy5MlRX1+fy+znn38+mpqacpmdoizLYvHrr+Qyu7KqOv4y+3e5hGJ1r97xvgNnuTwU6NbEHADJ2X333XOZm2VZLFq0KNauXZvL/HXr1uUyN1VNjQ1x26+vzmV2vwE1sfMeU3OJubKy8ug3YKBQBEpOzAHAfysUCnH00UfnMjvLsrj++utzO6NIW2tW1cY3vvjxXGYPHb59fOX//VsuswE2h5gDgLfI82zLYYcdFsVisdPnFovFmD17dm7vUUxWluUytnbF0vjJN74UEZ3/78rQEdvHiZ/+YqfPBbonMQcAXaBQKMSwYcNymV0sFmPSpEm5PAqiWCzGvHnzIsspjFLU3NQUL7/0Qi6za1csiwfvmZ3L7P4DB8Uue+6by2ygNMQcbIEsy6J+3dpo7uAmCYVCIaqqqryXAugyZWVlMW3atFxmNzc3x2uvvZZLKGZZFo2NjZ0+N2V1K5fFb67+cS6zx0yYFOMm7pzL7LLy8nyecwi8o0K2iX/VVldXl/daIBnFYjG++aVPxKqVy9v9bXX//v3j2GOPjbKyMkEHJC/LstzOyjU0NMR1113nrF8XKsvpMRA77fre+OTnz89lNvRUAwcOfNdjnJmDLVRsaenwB5BCoSDkgG6jUCjk9v9n1dXVMWvWrFxmr1+/Pu6+++5cZqesmMMZ1oiIl196Pv7tO/+8yccPrBkSJ5x6tj8rYSuJOQCgJMrKymLUqFG5zG5oaIgJEybkctavqakpXn311U6fm7J1a1bH3Gce3+Tj+w+siSceujd22u290btP3/wWBt2cmAMAup3q6uo4+OCDc5m9atWquOmmm3KZnWVZLu9P3NasrquN/7ji23HW1y+NkWPGR0VFZamXBEkScwAAm6F///5x0kkn5TJ7xYoVcfPNN+cye1t0+cVfiQ8c9o9xxHGnlHopkCQxBwCwGQqFQlRU5PMjVE1NTRxyyCG5zK6trY1HH300l9lbqrmpqUeciYS8iDkAgG1EVVVVjB8/PpfZK1eujMWLF+cye/369bFs2bJcZgMbJ+YAAHqAQYMG5Xb30FdffTX++Mc/bnR/no+4gJ5MzAEAsFVGjhwZ//t//++N7l+5cmXceuutXbgi6BnEHAAAW6WsrCyqq6s3ur+y0t0qIQ9lpV4AAAAAm0/MAQCQm3d8r1yhEBGFLlsLdDcuswQAIDePPvpoPPfccx3u+8yXvxk7jJvYxSuC7sOZOQAActPU1BQNDQ0d7uvbf0D06t2ni1cE3Yczc7CZGhvWx9JFr0VLS3O7fb17946BAweWYFUAAPQ0Yg4202svz4/LvvXlDvftvPPO8d73vreLVwQAQE/kMksAAIAEOTMHJZZlWdTX17/z3b4AYDNUV1dHRYUf86C787sctgENDQ1iDoBO4yHd0DOIOSixQqEQNTU1pV4GAACJEXOwDSgUPDAVAIDN4wYoAAAACRJzAAAACRJzAAAACRJzAAAACRJzAAAACXI3SwAAOl2WZdHY2BgtLS3t9hUKhejVp2+UlZWXYGXQfYg5AABycdNNN8XatWvbbR88bLv48reuiLJyMQdbw2WWAADkolgsbnRfWXm556zCVhJzsBlemf9izHvuiQ73jR49OgYNGtTFKwIAoKdymSVshvvvuj0eufdPHe6bNm1a9O/fv4tXBABAT+XMHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQIIqSr0AAAC6l6ampqirq4tisdhuX78BNTF8+x1KsCrofsQcAACdavny5XHrrbd2uG/awUfGYR86sYtXBN2TyywBAAAS5MwcbIJisSXuvv3GeP3ll9rt6927d+y8885RVVW1RbOzLIv6+vrIsmxrlwkAERFRXV0dFRV+zIPuzu9y2ATFYjHuvu3GWF+/tt2+3r17x3vf+96tmt/Q0CDmAOg0lZWVpV4C0AXEHJRYoVCImpqaUi8DAIDEiDnYBhQKhVIvAQCAxLgBCgAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAQILEHAAAnWbp0qXx+uuvd7hv5933ie1GjeniFUH3VVHqBQAA0H08++yzMXfu3A73feifPhNDhm3XxSuC7suZOQAAgAQ5Mwfv4rWXX4o7/utX0di4vt2+SZMmxY477liCVQEA0NOJOXgXq1fVxtOPPdDhvsGDB8eoUaO6eEUAAOAySwAAgCSJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgARVlHoB0NNlWRb19fWRZVmplwJAN1FdXR0VFV37Y16xWIwnn3wyli9f3m5fvwE1Me2QI6NPn35duibo7sQcbAMaGhrEHACdprKysstfM8uyePLJJ6OxsbHdvv4Da+Kwfzyxy9cE3Z2YgxIrFApRU1NT6mUAAJAYMQfv4L+u+2k8/ehfO9x3xBFHxODBgzvldQqFQqfMAQCg53ADFHgHK5cvjZXLl3S4r6amJnr16tXFKwIAgDeJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgASJOQAAgARVlHoBAACkbfny5fHYY49Fc3Nzu31Tp8+Ivd53YNcvCnoAMQcAwFZZt25dvPzyyx3uGzl6QkyasmfXLgh6CDEHHciyLFpamiPLsg73l5eXd/GKAACgLTEHHWhqbIj/95VPx5rVde32DRs2LGbNmhVVVVUlWBkAALxJzEEHsoioX7cmii0t7faVlZVFdXV11y8KAADewt0sAQAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEuQ5c1BiWZZFfX19ZFlW6qUA0E1UV1dHRYUf86C787sctgENDQ1iDoBOU1lZWeolAF1AzEGJFQqFqKmpKfUyAABIjJiDbUChUCj1EgBgizzwwAPx8ssvt9teKBTitC9/M7bfYVzXLwp6CDdAAQBgi61duzbWrFnT4b7h2+8QffsP6OIVQc8h5uBtmhobYnXtiujoLWxVVVXRq1evrl8UAAC8jcss4W2effyh+MXll0RE+5rbb7/9YtKkSV2/KAAAeBtn5uBtsrf859sVCgXvbwMAYJsg5gAAABIk5gAAABIk5gAAABIk5gAAABIk5gAAABIk5gAAABIk5gAAABIk5gAAABIk5gAAABJUUeoFAACQnizLolgsRpZlHe4vr6iIQqHQxauCnkXMAQCw2Zqbm+M3v/lNrF+/vt2+MRN2ilO/eGH07tOnBCuDnkPMAQCwRRoaGqJYLLbbXlZeFn369ivBiqBn8Z45eItFr74cS994tcN9w4YNi169enXxigAAoGPOzMFb/Nf1P4u5zzzebntZWVnMmjUrqquru35RAADQAWfmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEuRullBiWZZFfX19ZFlW6qUA0E1UV1dHRYUf86C787sctgENDQ1iDoBOU1lZWeolAF1AzEGJFQqFqKmpKfUyAABIjJiDbUChUCj1EgAASIwboAAAACRIzAEAsFmam5tj3bp1He7r1adv9O03oItXBD2TyywBANgsr7zyStx1110d7jv6I6fG1OkzunhF0DM5MwcAQKcpFLwXHLqKM3MQEc1NTfHYA3fHqpXL2+3r06dPjBkzJsrLy0uwMgAA6JiYg4hobFgfv73msigWW9rtGzx4cOy///4lWBUAAGycyywBAAASJOYAAAASJOYAAAASJOYAAAASJOYAAAASJOYAAAASJOYAAAASJOYAAAASJOYAAAASJOYAAAASJOYAANhkK1eujNra2g73jZmwU/QbUNOl64GerKLUCwAAIB0PPPBAvP766+22l5dXxKe+dGH07tOvBKuCnsmZOQAAgAQ5M0eP9/K85+OR++6MLCu22zdp0qQYM2ZMCVYFAADvTMzR4y167eV44O7fd7hv9OjRMW7cuK5dEAAAbAKXWQIAACRIzAEAACTIZZZQYlmWRX19fWRZVuqlANBNVFdXR0WFH/Ogu/O7HLYBDQ0NYg6ATlNZWVnqJQBdQMxBiRUKhaipqSn1MgAASIyYg21AoVAo9RIAAEiMG6AAAAAkSMwBAAAkSMwBAAAkSMwBAAAkyA1QAAB4Vy0tLTFv3rxYt25du30DBw2JnfeYGuXlHokAXUnMAQDwrpqamuLee+/t8Lmo240aG8ee8rkSrAp6NpdZAgAAJMiZOXq0//yPf4t5zz7ebnt5eXkccsghMWzYsK5fFAAAbAIxR4/29/kvxuLX/95ue6FQiJEjR0ZFhd8iAABsm1xmCQAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkCAxBwAAkKCKUi8AAIBt25IlS+LFF1+MLMva7Zs6fUbssud+JVgV4MwcAADvaMWKFfH88893uG+XPfaN3fZ+fxevCIhwZo4eqqO/WQQAgJSIOXqkNavq4iffPCfqVixrt2/06NExbdq0KC8vL8HKAABg04g5eqRisSWWL10U0cEZuoqKiujfv3+XrSXLsqivr3e2EIBOU11dHRUVfsyD7s7vctgGNDQ0iDkAOk1lZWWplwB0ATEHJVYoFKKmpqbUywAAIDFiDrYBhUKh1EsAACAxHk0AAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQIM+ZAwBgo+6///54/fXX222vqKyMj51xXoyeMKkEqwIixBwAAO9g6dKlUVtb2257WaEsJu68R1RVV3f9ooCIcJklPVBLc3M0NTZEZO33lZeXR3l5edcvCgAANpMzc/Q49991W9z2m2ujo5o75JBDYtSoUV2/KAAA2EzOzNHjtLS0RHNTY4f7nJkDACAVYg4AACBBYg4AACBBYg4AACBBYg4AACBBYg4AACBBYg4AACBBYg4AACBBYg4AACBBYg4AACBBFaVeAAAA254sy975gELXrAPYODEHAEA769evj5tvvjnWrl3bbt/k3feJD//TZ6KyqqoEKwM2EHNQYlmWRX19/bv/DSgAbKLq6uqoqNi6H/OKxWKsXr16I/N7xeBh223VfGDriTl6lJXLl8TaNavabS8UCtGvX78oLy8vwaoiGhoaxBwAnaaysrLUSwC6gJijR/n5pRfGotdebre9qqoqjjvuuCgUuv4NAIVCIWpqarr8dQEASJuYg7coRcyV8nUBAEiXRxMAAAAkSMwBAAAkSMwBAAAkSMwBAAAkSMwBAAAkSMwBAAAkSMwBAAAkSMwBAAAkSMwBAAAkSMwBANBGsViM5ubmDvdVVFZGRWVlF68I6EhFqRcAAMC25dlnn42HH364w30fO+O8mDRlry5eEdARZ+YAAGijWCxGS0tLh/sqKpyZg22FM3P0CI0N62P+i89Ew/r6dvv69OkTI0aMKMGqAABgy4k5eoRVtSviyu+d3+G+0aNHx/Tp07t4RQAAsHVcZgkAAJAgMQcAAJAgMQcAAJAgMQcAAJAgMQcAAJAgMQcAAJAgMQcAAJAgMQcAAJAgMQcAAJCgilIvAHq6LMuivr4+siwr9VIA6Caqq6ujosKPedDd+V0O24CGhgYxB0Cnqays3OKPXbNmTTQ0NLTbXigUYvCw7aKyqnprlgZ0IjEHJVYoFKKmpqbUywCAiIiYPXt2rFy5st32Xn36xpcvviLKyspLsCqgI2IOtgGFQqHUSwCATeLPLNh2iDm6vYVzn41nHn+ow3077rhjjBo1qotXBAAAW0/M0e298PRjcfdtv+1w35QpU2L48OFdvCIAANh6Hk0AAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQIDEHAACQoIpSLwAAgNJrbm6ON954I5qamtrtGzhoSIydODkKUSjByoCNEXMAAMTatWtj9uzZHe7babe94/hPnNXFKwLejcssAQAAEuTMHN1WlmVx26+vjhef/lu7fVVVVbHffvtF//79S7AyAADYemKObu3Jh++LFcsWt9teUVERkyZNikLBtf8AAKTJZZYAAAAJEnMAAAAJEnMAAAAJEnMAAAAJEnMAAAAJEnMAAAAJ8mgCKLEsy6K+vj6yLCv1UgDoJqqrq6Oiwo950N35XQ7bgIaGBjEHQKeprKws9RKALiDmoMQKhULU1NSUehkAACRGzME2oFAolHoJAAAkRswBAPRwixcvjldeeaXDfXu974DYade9unhFwKYQcwAAPdyrr74aTzzxRIf79j/0qBi74+QuXhGwKTyaAAAAIEHOzNEtLVv8elz/0+/FqtoV7fbtuOOOsdtuu5VgVQAA0HnEHN1SY0NDvPzSCx3u69OnTwwdOrSLVwQAAJ3LZZYAAAAJEnMAAAAJcpklPc6CBQti+fLlG92//fbbx157uQUzAADbNjFHj7NmzZpYs2bNRvc3NzfHwIEDc3nt/v37x7Bhw3KZDQBAzyLm4G2WLFkSd911Vy6z3/Oe98S0adNymV1WVhbl5eW5zAYAYNsj5qALzZs3L+bPn5/L7F133TWmTp2ay2wAALY9Yo5uadDQ4fGxM86Lm/798lizuq7Uy2mVZVm0tLTkMnvBggVRV5fP5zphwoSYMGFCLrMBANgyYo5uqXefvrHbPtPi2ccfigVzn4nlSxaVekm5W7VqVaxatSqX2RUVFVFZWZnL7Jqamujfv38uswEAujMxR7dVKBTiI5/6fPzxd9fHnbfcsMkfl2XFKBaLOa4sPfPmzYt58+blMnu//faLXXbZJZfZZWVlUSgUcpkNAFBqYo5u76Aj/lfsf+hRm3z80489EL/++Q9zXBFv9cgjj8Tf/va3XGZPmzYtJk6cmMtsAIBSK2RZlm3KgXm9Fwe2NSuWLo6Xnn8yl9nzX3gmHr73T7nMpr0RI0bEgAEDcpk9ZcqUGDp0aC6zAbpKlmXx0EMPxWuvvRYrVqxos69Xn75x9EdOjV323Df6DcjnkT3Axm3Ko7KcmYO3GTxsRAwedmgus/v2HxhLF7+Wy+y6lctj5bIlucxO1eLFi2Px4sW5zB44cGA0NzfnMnvQoEFRXV2dy2yAt1uwYEGHz1+tqqqOqfsfEoWyshKsCtgUYg660C577hu77LlvLrPv+cN/xq03XJXL7E08gd+jPPLII7nNnjVrVowaNSq3+d5HCADdg5iDbuL9Bx0ee73vgFxm//n3N8VfZv8ul9m0d9ddd+X2APiZM2e6PBQAugkxB91EVXWvqKrulcvs3afuHzWDh+Uy++lH74/5Lz6Ty+xUNTY25jb7ySefjN69e+cye4899og+ffrkMhsAaE/MAe9q3MTJMW7i5FxmtzQ3RX392lxm165YFvVr278PpCebP39+brO322673G44U1NTk9vZSgBIlZgDSuqgI4+Ng448NpfZN1z5A3cP7UJ33nlnbrOPP/74XB8u732EAKRIzAHd1pHHfzxmHH1CLrNv/PfL48Wn83k+Hu3ddtttUZbDHfUKhUIcddRRuV16CgB5EnNAt9VvwMDcno207wcOi3ETd8ll9qP33RnLly7KZXaq1q7N51LciIgnnngiKisrO31uoVCI3XffPSoq/FELQD78CQOwBfbcd3pus1csXRSR02V/dSuWRXNzUy6zU/X000/nMresrCzGjBmTWyj279/f5aEAPZyYA9jGnHDq2bnN/vFFX4pX5r+Q23z+R7FYjN/97ne5zK6oqIiTTjrJWT+AHs6fAgDbmDzPtnz0M1+KpsaGXGZf+6NvxrIlb+Qym7aam5vjd7/7XS7/rlRVVcWRRx6Zy3sUAehcYg6gBxk6fPvcZr/voMNjdd3KXGY/eM/sWF+/LpfZqaqtrc1lbnl5eTz11FO5hGJlZWVMnjzZ5aEAnUTMAdApDjz8w7nMzbIsFrz4bNSuWJrL/NV1KyPLslxmp6ilpSUefvjhXGb37t07xowZk8vssrKy6NWrl1AEehQxB8A2rVAoxJlf+25EDsFVzLL4xhdOjtV1tZ0+m/bq6+vj+uuvz2X2gAED4rjjjstlNsC2qpBt4l9H1tXV5b0WAOhSWZbFqwvnRUtLc6fPLra0xJWXXhCNDes7fTbtlZeXx5AhQ3KZPWDAgDjwwANzmV1KdXV1cc8998SyZcuiWCy22bfX+w6IA2YdE6PG7uhsJ5TIwIHv/nglZ+YA6LEKhUKMHv+eXGYXW1pi6vRDc7nhTLGlJR574O52P4D3ZC0tLbFkyZJcZq9duzaef/75XGb36dMnt0tP301zc/NGv2YDagbHDuMmdvGKgM0l5gAgB2Xl5XHMSaflMruxYX28+Ozj+dyZNMuifl1+D2lP0dq1a+Pee+/NZfawYcNi+PDhucwuKyuLqqqqXGYD2wYxBwCJqayqjq9+5+e5zF63dnVc9PlTolhsyWU+bS1dujR++ctf5jJ7hx12iJkzZ+YyG9g2iDkASEyhUIjynB4Y3qdv/zj1ixfmcsOZtWtWxy//9ZJOn5u6vO6mumTJkrj99ts3ur+5ufPfKwp0LTEHALQqr6iISVP2zGX2urWrY499p+cSig0N6+P5Jx/p9Lkpa2hoiNdff73UywByJOYAgC7Rp2//+KfPfiWX2cuWvBGX/vNTucwuZlk0NzXmMhtga4g5ACB5Q4ZtFxf8+LpcZi96dWH86KIv5jIbYGt4zhwAwDuoX7c25j7zeC6zl7zxavzhpl/kMntLHX3iqTFxlz1i5OjxpV4K9GieMwcAsJV69+kbu0/9h1xmL3rt5Vjw4jO5zF67ZlW8unDeJh9fWVUdEyZNiV33fn8MHjoilzUBncuZOQCAbuiFpx+Lq77/L5t8/JDh28c537o8CoVCjqsCNtWmnJkTcwAA3VBLS3M0rF+/yceXlRWiV+++Oa4I2BwuswQA6KHKyyuiT99+pV4GkKOyUi8AAACAzSfmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAEiTmAAAAElTIsiwr9SIAAADYPM7MAQAAJEjMAQAAJEjMAQAAJEjMAQAAJEjMAQAAJEjMAQAAJEjMAQAAJEjMAQAAJEjMAQAAJOj/A/nFh0NABMkeAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_camera_parameters(R, t, K, filename='camera_params.npz'):\n",
        "    \"\"\"Save camera parameters for later use\"\"\"\n",
        "    np.savez(filename, R=R, t=t, K=K)\n",
        "    print(f\"âœ“ Camera parameters saved to {filename}\")\n",
        "\n",
        "\n",
        "def load_camera_parameters(filename='camera_params.npz'):\n",
        "    \"\"\"Load saved camera parameters\"\"\"\n",
        "    data = np.load(filename)\n",
        "    return data['R'], data['t'], data['K']\n",
        "\n",
        "\n",
        "def visualize_camera_frustum(R, t, K, image_size, plane_points):\n",
        "    \"\"\"Visualize camera frustum and plane in 3D\"\"\"\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Camera center in world coordinates\n",
        "    C = -R.T @ t\n",
        "\n",
        "    # Draw camera\n",
        "    ax.scatter(*C, c='red', s=100, marker='o', label='Camera')\n",
        "\n",
        "    # Draw plane\n",
        "    plane_points_3d = np.vstack([plane_points, plane_points[0]])\n",
        "    ax.plot(plane_points_3d[:, 0], plane_points_3d[:, 1],\n",
        "            plane_points_3d[:, 2], 'b-', linewidth=2, label='Plane')\n",
        "\n",
        "    # Draw camera frustum\n",
        "    h, w = image_size\n",
        "    corners_2d = np.array([[0, 0], [w, 0], [w, h], [0, h]], dtype=np.float32)\n",
        "\n",
        "    # Set axis labels and limits\n",
        "    ax.set_xlabel('X')\n",
        "    ax.set_ylabel('Y')\n",
        "    ax.set_zlabel('Z')\n",
        "    ax.legend()\n",
        "    ax.set_title('Camera Pose Visualization')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def create_ar_video(video_path, corners_list, output_path='ar_video.mp4'):\n",
        "    \"\"\"Process video and add AR objects (bonus feature)\"\"\"\n",
        "    print(\"Video processing feature - for advanced users\")\n",
        "    # This would require frame-by-frame processing\n",
        "    # Left as an exercise for students wanting extra credit\n",
        "    pass"
      ],
      "metadata": {
        "id": "njZyZJ11XruV"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}